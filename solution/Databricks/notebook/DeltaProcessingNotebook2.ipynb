{"cells":[{"cell_type":"code","source":["TaskObject = \"\"\"{\"TaskInstanceId\": 37,\n    \"TaskMasterId\": -1000,\n    \"TaskStatus\": \"InProgress\",\n    \"TaskType\": \"Azure Storage to Azure Storage\",\n    \"Enabled\": 1,\n    \"ExecutionUid\": \"9d06d8c7-9184-4bee-9605-62142e22781c\",\n    \"NumberOfRetries\": 0,\n    \"DegreeOfCopyParallelism\": 1,\n    \"KeyVaultBaseUrl\": \"https://ads-stg-kv-ads-r6p7.vault.azure.net/\",\n    \"ScheduleMasterId\": \"-4\",\n    \"TaskGroupConcurrency\": \"10\",\n    \"TaskGroupPriority\": 0,\n    \"TaskExecutionType\": \"ADF\",\n    \"ExecutionEngine\": {\n        \"EngineId\": -2,\n        \"EngineName\": \"adsstgsynwadsr6p7\",\n        \"SystemType\": \"Synapse\",\n        \"ResourceGroup\": \"gfuat2\",\n        \"SubscriptionId\": \"035a1364-f00d-48e2-b582-4fe125905ee3\",\n        \"ADFPipeline\": \"GPL_SparkNotebookExecution_Azure\",\n        \"EngineJson\": {\"endpoint\": \"https://adsstgsynwadsr6p7.dev.azuresynapse.net\", \"DeltaProcessingNotebook\": \"DeltaProcessingNotebook\", \"PurviewAccountName\": \"adsstgpuradsr6p7\", \"DefaultSparkPoolName\":\"adsstgsynspads\"},\n        \"TaskDatafactoryIR\": \"Azure\",\n        \"JsonProperties\": {\n            \"endpoint\": \"https://adsstgsynwadsr6p7.dev.azuresynapse.net\",\n            \"DeltaProcessingNotebook\": \"DeltaProcessingNotebook\",\n            \"PurviewAccountName\": \"adsstgpuradsr6p7\",\n            \"DefaultSparkPoolName\": \"adsstgsynspads\"\n        }\n    },\n    \"Source\": {\n        \"System\": {\n            \"SystemId\": -4,\n            \"SystemServer\": \"https://adsstgdlsadsr6p7adsl.dfs.core.windows.net\",\n            \"AuthenticationType\": \"MSI\",\n            \"Type\": \"ADLS\",\n            \"Username\": null,\n            \"Container\": \"datalakeraw\"\n        },\n        \"Instance\": {\n            \"SourceRelativePath\": \"samples/SalesLT_Customer_CDC/\",\n            \"TargetRelativePath\": \"/Tests/Azure Storage to Azure Storage/-1000/\"\n        },\n        \"DataFileName\": \"SalesLT.Customer*.parquet\",\n        \"DeleteAfterCompletion\": \"false\",\n        \"MaxConcurrentConnections\": 0,\n        \"Recursively\": \"false\",\n        \"RelativePath\": \"samples/SalesLT_Customer_CDC/\",\n        \"SchemaFileName\": \"SalesLT.Customer*.json\",\n        \"Type\": \"Parquet\",\n        \"WriteSchemaToPurview\": \"Disabled\"\n    },\n    \"Target\": {\n        \"System\": {\n            \"SystemId\": -4,\n            \"SystemServer\": \"https://adsstgdlsadsr6p7adsl.dfs.core.windows.net\",\n            \"AuthenticationType\": \"MSI\",\n            \"Type\": \"ADLS\",\n            \"Username\": null,\n            \"Container\": \"datalakeraw\"\n        },\n        \"Instance\": {\n            \"SourceRelativePath\": \"samples/SalesLT_Customer_CDC/\",\n            \"TargetRelativePath\": \"/Tests/Azure Storage to Azure Storage/-1000/\"\n        },\n        \"DataFileName\": \"SalesLT.Customer\",\n        \"DeleteAfterCompletion\": \"false\",\n        \"MaxConcurrentConnections\": 0,\n        \"Recursively\": \"false\",\n        \"RelativePath\": \"/Tests/Azure Storage to Azure Storage/-1000/\",\n        \"SchemaFileName\": \"SalesLT.Customer.json\",\n        \"Type\": \"Delta\",\n        \"WriteSchemaToPurview\": \"Disabled\"\n    },\n    \"TMOptionals\": {\n        \"CDCSource\": \"Disabled\",\n        \"Purview\": \"Disabled\",\n        \"QualifiedIDAssociation\": \"TaskMasterId\",\n        \"SparkTableCreate\": \"Disabled\",\n        \"SparkTableDBName\": \"\",\n        \"SparkTableName\": \"\",\n        \"UseNotebookActivity\": \"Disabled\"\n    }\n}\"\"\""],"metadata":{"tags":["parameters"],"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85f11718-ce39-480b-952c-6eca2e3c0c79"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import random\nimport json\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom datetime import date\nfrom datetime import datetime\n\nsession_id = random.randint(0,1000000)\n\nTaskObjectJson = json.loads(TaskObject)\nSource = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\nSchema = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\nTarget = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n\n\nSource = Source + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['DataFileName']\nSchema = Schema + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['SchemaFileName']\nTarget = Target + TaskObjectJson['Target']['Instance']['TargetRelativePath'] + \"/\" + TaskObjectJson['Target']['DataFileName']\n\n\n#remove any double slashes\nSource = Source.replace('//', '/')\nSchema = Schema.replace('//', '/')\nTarget = Target.replace('//', '/')\n\n#get source and target data types\nSourceDT = TaskObjectJson['Source']['Type']\nTargetDT = TaskObjectJson['Target']['Type']\n\n\n#add abfss\nSource = \"abfss://\" + Source\nSchema = \"abfss://\" + Schema\nTarget = \"abfss://\" + Target\n\nnow = datetime.now()\n\nSource = Source.replace(\"{yyyy}\", \"%Y\")\nSource = Source.replace(\"{MM}\", \"%m\")\nSource = Source.replace(\"{dd}\", \"%d\")\nSource = Source.replace(\"{hh}\", \"%H\")\nSource = Source.replace(\"{mm}\", \"%M\")\nSource = now.strftime(Source)\n\nTarget = Target.replace(\"{yyyy}\", \"%Y\")\nTarget = Target.replace(\"{MM}\", \"%m\")\nTarget = Target.replace(\"{dd}\", \"%d\")\nTarget = Target.replace(\"{hh}\", \"%H\")\nTarget = Target.replace(\"{mm}\", \"%M\")\nTarget = now.strftime(Target)\n\nSchema = Schema.replace(\"{yyyy}\", \"%Y\")\nSchema = Schema.replace(\"{MM}\", \"%m\")\nSchema = Schema.replace(\"{dd}\", \"%d\")\nSchema = Schema.replace(\"{hh}\", \"%H\")\nSchema = Schema.replace(\"{mm}\", \"%M\")\nSchema = now.strftime(Schema)\n\nprint (\"Source: \" + Source)\nprint (\"Schema: \" + Schema)\nprint (\"Target: \" + Target)\n\n"],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf2fb0c-4af8-4b4b-9712-09e06c6a66f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Source: abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.parquet\nSchema: abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.json\nTarget: abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/Tests/Azure Storage to Azure Storage/-1000/SalesLT.Customer\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Source: abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.parquet\nSchema: abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.json\nTarget: abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/Tests/Azure Storage to Azure Storage/-1000/SalesLT.Customer\n"]}}],"execution_count":0},{"cell_type":"code","source":["Source = \"abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.parquet\"\n\nSchema = \"abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.json\"\n\nTarget = \"abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/Tests/Azure Storage to Azure torage/-1000/SalesLT.Customer\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffdebcd2-8d12-4980-b119-ade79b167c6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: 'abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.json'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: 'abfss://datalakeraw@adsstgdlsadsr6p7adsl.dfs.core.windows.net/samples/SalesLT_Customer_CDC/SalesLT.Customer*.json'"]}}],"execution_count":0},{"cell_type":"code","source":["schema = spark.read.load(Schema, format='json', multiLine=True)\nschema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91474df9-90a4-413e-8835-1805e34adb52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-3804581651887330>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mSchema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'json'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmultiLine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    175\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o615.load.\n: Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:51)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:593)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1847)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:227)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:142)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:818)\n\tat scala.collection.immutable.List.flatMap(List.scala:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:816)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:626)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:453)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:368)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:324)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:324)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\n\tat sun.reflect.GeneratedMethodAccessor1405.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:70)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 28 more\n","errorSummary":"Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-3804581651887330>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mSchema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'json'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmultiLine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    175\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o615.load.\n: Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:51)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:593)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1847)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:227)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:142)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:818)\n\tat scala.collection.immutable.List.flatMap(List.scala:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:816)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:626)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:453)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:368)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:324)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:324)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\n\tat sun.reflect.GeneratedMethodAccessor1405.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:70)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 28 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["try:\n    print(\"Schema found - using schema json to find PK\")\n    schema = spark.read.load(Schema, format='json', multiLine=True)\n    #convert it into a list so we can loop it using python rules\n    schema = schema.collect()\n    #loop through each column to find the primary key column\n    for col in schema:\n        if col.PKEY_COLUMN:\n            print(col.COLUMN_NAME)\n            primaryKey = col\n            mergeCondition = \"oldData.\" + primaryKey.COLUMN_NAME + \" = newData.\" + primaryKey.COLUMN_NAME\n            break\nexcept:\n    print(\"Schema json not found - assuming source dataframe first column is PK\")\n    df = spark.read.load(Source, format=SourceDT)\n    primaryKey = df.columns[0]\n    print(primaryKey)\n    mergeCondition = \"oldData.\" + primaryKey + \" = newData.\" + primaryKey\n\n#set up the merge condition used in the next code block\nprint(mergeCondition)"],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3f1d1e1-248c-448c-bef5-0f2f41941c15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"application/vnd.livy.statement-meta+json":{"session_id":82,"spark_pool":"dlzstgsynspads","state":"finished","execution_finish_time":"2022-03-01T20:42:29.4180882Z","execution_start_time":"2022-03-01T20:42:15.9281891Z","queued_time":"2022-03-01T20:42:15.8040424Z","livy_statement_state":"available","session_start_time":null,"statement_id":5},"text/plain":["StatementMeta(dlzstgsynspads, 82, 5, Finished, Available)"]},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":82,"spark_pool":"dlzstgsynspads","state":"finished","execution_finish_time":"2022-03-01T20:42:29.4180882Z","execution_start_time":"2022-03-01T20:42:15.9281891Z","queued_time":"2022-03-01T20:42:15.8040424Z","livy_statement_state":"available","session_start_time":null,"statement_id":5},"text/plain":["StatementMeta(dlzstgsynspads, 82, 5, Finished, Available)"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"CustomerID","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CustomerID"]}}],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\nimport pandas as pd\nif(TaskObjectJson['TMOptionals']['CDCSource'] == 'Enabled'):\n    print(\"CDC Source\")\n    df = spark.read.load(Source, format=SourceDT)\n    #these are our cdc specific columns\n    cdcCols = [\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$update_mask\", \"__$operation\", \"__$command_id\"]\n    #we are dropping all of the $ / _ as they cause issues with spark SQL functions -> this may be changed to just remove the first 3 chars of each of the cdcCols (__$)\n    cdcColsToDrop = [\"startlsn\", \"endlsn\", \"seqval\", \"updatemask\", \"operation\", \"commandid\"]\n    #these are columns we want to convert from binary data types to string as dataframes do not play nice with them currently\n   #colsToString = [\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$update_mask\"]\n    colsToString = [\"startlsn\", \"endlsn\", \"seqval\", \"updatemask\"]\n\n    for col in cdcCols:\n        new_col = col.replace('_','')\n        new_col = new_col.replace('$','')\n        df = df.withColumnRenamed(col, new_col)\n\n    for col in colsToString: \n        try:\n            df = df.withColumn(col, hex(col))\n        except:\n            print(\"Error converting the column \" + col)\n\n    #convert to pandas dataframe so we can do more manipulation\n    pdf = df.toPandas()\n\n    #we want to sort by our start lsn and then by the seqval so that we can drop everything except the most recent database change for every unique row\n    try:\n        #columns we are sorting by, the LSN and then the sequence value - ensure the latest is at the bottom of the table\n        pdf = pdf.sort_values(by=[\"startlsn\", \"seqval\"])\n    except:\n        print(\"error in finding valid sorting columns - skipping.\")\n    pdf_dedupe = pdf.drop_duplicates(subset=[primaryKey.COLUMN_NAME], keep='last', inplace=False)\n\n    df = spark.createDataFrame(pdf_dedupe)\n\n    #operation 1 is equal to delete, the other 3 operations (inserts old and new / upserts) can be done together\n    dfDeletes = df.filter(\"operation == 1\")\n    dfUpserts = df.filter(\"operation != 1\")\n    #We want to sort our columns by our primary key now that we have only the latest actions\n    dfDeletes = dfDeletes.sort(primaryKey.COLUMN_NAME)\n    dfUpserts = dfUpserts.sort(primaryKey.COLUMN_NAME)\n\n    #drop unwanted columns -> not needed for our delta table as they are cdc specific\n    for col in cdcColsToDrop:\n        try:\n            dfDeletes = dfDeletes.drop(col)\n            dfUpserts = dfUpserts.drop(col)\n        except:\n            print(\"Error dropping the column \" + col)\n\n    sql = 'describe detail \"' + Target + '\"'\n    try:\n        if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n            print(\"Table already exists. Performing Merge\")\n            olddt = DeltaTable.forPath(spark, Target)  \n            olddt.alias(\"oldData\").merge(\n                dfUpserts.alias(\"newData\"),\n                mergeCondition) \\\n            .whenMatchedUpdateAll() \\\n            .whenNotMatchedInsertAll() \\\n            .execute()\n        else:\n            print(\"Table does not exist. No error, creating new Delta Table.\")    \n            dfUpserts.write.format(\"delta\").save(Target)\n    except: \n        print(\"Table does not exist, error thrown. Creating new Delta Table. Note - this error can be that no file is found.\") \n        dfUpserts.write.format(\"delta\").save(Target)\n\n    olddt = DeltaTable.forPath(spark, Target)  \n    olddt.alias(\"oldData\").merge(\n    dfDeletes.alias(\"newData\"),\n    mergeCondition) \\\n    .whenMatchedDelete() \\\n    .execute()  \n\nelse:\n    print(\"Non CDC Source\")\n    if(TargetDT == 'Delta'):\n        print(\"SourceDT = \" + SourceDT + \", TargetDT = Delta.\")\n        df = spark.read.load(Source, format=SourceDT)\n        sql = 'describe detail \"' + Target + '\"'\n        try:\n            if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n                print(\"Table already exists. Performing Merge\")\n                olddt = DeltaTable.forPath(spark, Target)  \n                olddt.alias(\"oldData\").merge(\n                    df.alias(\"newData\"),\n                    mergeCondition) \\\n                .whenMatchedUpdateAll() \\\n                .whenNotMatchedInsertAll() \\\n                .execute()\n            else:\n                print(\"Table does not exist. No error, creating new Delta Table.\")    \n                df.write.format(\"delta\").save(Target)\n        except:\n            print(\"Table does not exist. Creating new Delta Table.\")    \n            df.write.format(\"delta\").save(Target)\n    elif(TargetDT == 'Parquet' and SourceDT == 'Delta'):\n        print(\"SourceDT = Delta, TargetDT = Parquet.\")\n        df = spark.read.format(\"delta\").load(Source)\n        df.write.format(\"parquet\").mode(\"overwrite\").save(Target) "],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cd2a0b6-3c93-4fe0-8f85-fb0250305e04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"application/vnd.livy.statement-meta+json":{"session_id":77,"spark_pool":"dlzstgsynspads","state":"finished","execution_finish_time":"2022-02-28T21:55:18.8952766Z","execution_start_time":"2022-02-28T21:54:59.2864675Z","queued_time":"2022-02-28T21:54:59.0347829Z","livy_statement_state":"available","session_start_time":null,"statement_id":36},"text/plain":["StatementMeta(dlzstgsynspads, 77, 36, Finished, Available)"]},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":77,"spark_pool":"dlzstgsynspads","state":"finished","execution_finish_time":"2022-02-28T21:55:18.8952766Z","execution_start_time":"2022-02-28T21:54:59.2864675Z","queued_time":"2022-02-28T21:54:59.0347829Z","livy_statement_state":"available","session_start_time":null,"statement_id":36},"text/plain":["StatementMeta(dlzstgsynspads, 77, 36, Finished, Available)"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Table already exists. Performing Merge","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Table already exists. Performing Merge"]}}],"execution_count":0},{"cell_type":"code","source":["#This checks if the user wants to save the sink as a persistent table\nif(TaskObjectJson['TMOptionals']['SparkTableCreate'] == 'Enabled'):\n    print(\"Creating Spark Table\")\n    df = spark.read.load(Target, format='delta')\n    #targetDB = 'testdb2'\n    #targetTable = 'test2'\n    #SynapseSnapshots/Workspacename/dbname/tablename.parquet\n    #SynapseTarget = Target + '/'+ targetDB + '/' + targetTable\n    #SynapseTarget = 'abfss://datalakeraw@' + TaskObjectJson['TMOptionals']['PersistentStorage']\n    targetDB = TaskObjectJson['TMOptionals']['SparkTableDBName']\n    targetTable = TaskObjectJson['TMOptionals']['SparkTableName']\n    #if the target datatype is parquet then we do not need to create a copy of the data - we can use the recently saved sink target\n    if (TargetDT == 'Parquet'):\n        SnapshotTarget = Target\n    else:\n        SnapshotTarget = Target + '/'+ TaskObjectJson['TMOptionals']['SparkTableDBName'] + '/' + TaskObjectJson['TMOptionals']['SparkTableName']\n        #we need to update the parquet file - this is not very efficient but there isnt a current better way as delta tables are not supported for persistent tables\n        df.write.format(\"parquet\").mode(\"overwrite\").save(SnapshotTarget)\n\n\n    #we need to make the DB and table lowercase as synapse persistent tables dont identify them as different identities\n    targetDB = targetDB.lower()\n    targetTable = targetTable.lower()\n\n    #check if the specified DB / table exists - if so only do required actions.\n    dbList = spark.catalog.listDatabases()\n    dbExists = False\n    for db in dbList:\n        if (db.name == targetDB):\n            dbExists = True\n            break\n    if (dbExists):\n        print(\"DB Exists\")\n        tableExists = False\n        spark.catalog.setCurrentDatabase(targetDB)\n        tableList = spark.catalog.listTables()\n        for table in tableList:\n            if (table.name == targetTable):\n                tableExists = True\n                break\n        if (tableExists):\n            print(\"Table exists - nothing needed to be done\")\n            spark.catalog.refreshTable(targetTable)\n        else:\n            print(\"Table doesnt exist - creating\")\n            spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\n    else:\n        print(\"DB Doesnt exist - creating DB and table\")\n        createDBString = \"CREATE DATABASE \" + targetDB \n        spark.sql(createDBString)\n        spark.catalog.setCurrentDatabase(targetDB)\n        spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\nelse:\n    print(\"Skipping Spark Table creation\")\n\n#%%sql\n#CREATE TABLE testdb.dbo.test\n#USING PARQUET OPTIONS ('path'= 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT.Customer.chunk_2.parquet', 'inferschema'=true);\n#select * from testdb.dbo.test limit 10\n\n    "],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00792c35-2d49-4de9-92a5-061aaab187b2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#olddt.history().show(20, 1000, False)\n#display(spark.read.format(\"delta\").load(Target))\n#spark.sql(\"CREATE TABLE SalesLTCustomer USING DELTA LOCATION '{0}'\".format(TargetFile))"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ead9f17e-429e-4d4f-9351-d289ddc5dac0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Execute Upsert\n#(old_deltaTable\n# .alias(\"oldData\") \n# .merge(newIncrementalData.alias(\"newData\"), \"oldData.id = newData.id\")\n# .whenMatchedUpdate(set = {\"name\": col(\"newData.name\")})\n# .whenNotMatchedInsert(values = {\"id\": col(\"newData.id\"), \"name\":\n#                                col(\"newData.name\")})\n# .execute()\n#)\n#\n# Display the records to check if the records are Merged\n#display(spark.read.format(\"delta\").load(Target))"],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90410408-ce10-4efc-a656-0bea3c970785"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#olddt.history().show(20, 1000, False)"],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ffbcb73-fc3d-4daa-85cf-36858fd9583f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#########################\n#NOTE -> This is an alternate way of upserting into delta table. Using manual method of getting each column required from the schema / dataframe and then creating a dictionary to use for the upsert.\n#           Currently not using this however it does work. Would advise to change the script to just create a dictionary and insert that instead of creating a string to convert into a dict.\n#########################\n#from delta.tables import *\n#df = spark.read.load(Source, format='parquet')\n#updatecols = []\n#insertcols = []\n#for col in schema:\n#    updatecols.append(col.COLUMN_NAME)\n#\n#for col in df.dtypes:\n#    insertcols.append(col[0])\n#\n#creating a string to be converted to dictionary \n#note -> can easily re-write this as just a dictionary if end up using this method.\n#updatestring = '{'\n#insertstring = '{'\n#\n#Go through each column in the schema to check what needs to be updated\n#for col in updatecols:\n#    updatestring = updatestring + '\"' + col + '\": \"newData.' + col +'\", '\n#updatestring = updatestring[:-2]\n#updatestring = updatestring + '}'\n#\n#Go through the new data to check what columns need to be inserted\n#for col in insertcols:\n#    insertstring = insertstring + '\"' + col + '\": \"newData.' + col +'\", '\n#insertstring = insertstring[:-2]\n#insertstring = insertstring + '}'\n#\n#print(updatestring)\n#print(insertstring)\n#\n#convert to dict\n#res = json.loads(updatestring)\n#res2 = json.loads(insertstring)\n#\n#sql = 'describe detail \"' + Target + '\"'\n#try:\n#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n#        print(\"Table already exists. Performing Merge\")\n#        olddt = DeltaTable.forPath(spark, Target)  \n#        olddt.alias(\"oldData\").merge(\n#            df.alias(\"newData\"),\n#            mergeCondition) \\\n#        .whenMatchedUpdate(set = res) \\\n#        .whenNotMatchedInsert(values = res2) \\\n#        .execute()\n#except:\n#    print(\"Table does not exist.\")    \n#    df.write.format(\"delta\").save(Target)"],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e4b1976-3151-4fc0-8fff-dcfb2e10470d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#from delta.tables import * \n#import pandas as pd\n#Source = 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT.Customer.chunk_1.parquet'\n#Target = 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT_Customer_Delta/SalesLT.Customer'\n#mergeCondition = \"oldData.\" + \"CustomerID\" + \" = newData.\" + \"CustomerID\"\n\n#df = spark.read.load(Source, format='parquet')\n\n#these are our cdc specific columns\n#cdcCols = ['__$start_lsn', '__$end_lsn', '__$seqval', '__$operation', '__$update_mask', '__$command_id']\n\n#convert to pandas dataframe so we can do more manipulation\n#pdf = df.toPandas()\n#we want to sort by our start lsn and then by the seqval so that we can drop everything except the most recent database change for every unique row\n#try:\n    #columns we are sorting by, the LSN and then the sequence value - ensure the latest is at the bottom of the table\n#    pdf = pdf.sort_values(by=['__$start_lsn', '__$seqval'])\n#except:\n#   print(\"error in finding valid sorting columns - skipping.\")\n\n#df = pdf.drop_duplicates(subset=['CustomerID'], keep='last', inplace=False)\n#df_dedupe = pdf.drop_duplicates(subset=[primaryKey.COLUMN_NAME], keep='last', inplace=False)\n\n#df = spark.createDataFrame(df)\n#dfDeletes = df.filter(\"CustomerID < 100\")\n#dfUpserts = df.filter(\"CustomerID >= 100\")\n#dfDeletes = df.filter(\"__$operation == 1\")\n#dfUpserts = df.filter(\"__$operation != 1\")\n\n#drop unwanted columns -> not needed for our delta table as they are cdc specific\n#for col in cdcCols:\n#    try:\n#        dfDeletes = dfDeletes.drop(col)\n#        dfUpserts = dfUpserts.drop(col)\n#    except:\n#        print(\"Error dropping the column \" + col)\n\n#sql = 'describe detail \"' + Target + '\"'\n#try:\n#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n#        print(\"Table already exists. Performing Merge\")\n#        olddt = DeltaTable.forPath(spark, Target)  \n#        olddt.alias(\"oldData\").merge(\n#            dfUpserts.alias(\"newData\"),\n#           mergeCondition) \\\n#        .whenMatchedUpdateAll() \\\n#        .whenNotMatchedInsertAll() \\\n#        .execute()\n#    else:\n#        print(\"Table does not exist. No error, creating new Delta Table.\")    \n#       dfUpserts.write.format(\"delta\").save(Target)\n#except: \n#    print(\"Table does not exist, error thrown. Creating new Delta Table.\")    \n#    dfUpserts.write.format(\"delta\").save(Target)\n\n#olddt = DeltaTable.forPath(spark, Target)  \n#olddt.alias(\"oldData\").merge(\n#dfDeletes.alias(\"newData\"),\n#mergeCondition) \\\n#.whenMatchedDelete() \\\n#.execute()  \n#display(dfDeletes)\n\n#df.write.format(\"delta\").save(Target)\n"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13f9453e-edf0-4d8a-b9b9-098b8fdfac1d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"application/vnd.livy.statement-meta+json":{"session_id":11,"spark_pool":"arkstgsynspads","state":"finished","execution_finish_time":"2022-04-04T03:46:37.4935481Z","execution_start_time":"2022-04-04T03:46:14.1251549Z","queued_time":"2022-04-04T03:46:14.012733Z","livy_statement_state":"available","session_start_time":null,"statement_id":35},"text/plain":["StatementMeta(arkstgsynspads, 11, 35, Finished, Available)"]},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":11,"spark_pool":"arkstgsynspads","state":"finished","execution_finish_time":"2022-04-04T03:46:37.4935481Z","execution_start_time":"2022-04-04T03:46:14.1251549Z","queued_time":"2022-04-04T03:46:14.012733Z","livy_statement_state":"available","session_start_time":null,"statement_id":35},"text/plain":["StatementMeta(arkstgsynspads, 11, 35, Finished, Available)"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"error in finding valid sorting columns - skipping.\nTable already exists. Performing Merge","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["error in finding valid sorting columns - skipping.\nTable already exists. Performing Merge"]}}],"execution_count":0},{"cell_type":"code","source":["#from delta.tables import * \n#import pandas as pd\n\n#df = spark.createDataFrame([\"0x0000019600000178002D\",\"0x0000019600000178002D\",\"0x0000019600000178002D\", \"0x0000019600000178002A\"], \"string\").toDF(\"hex\")\n#hex2 = ['0x00000194000000A80002', '0x00000A94000000A80002', '0x00000194000000B80004', '0x00000194000000B80000']\n\n#pdf = df.toPandas()\n#pdf['hex2'] = hex2\n\n\n#df = spark.createDataFrame(pdf)\n#df_dedupe = df.dropDuplicates('hex')\n#df_dedupe = df.dropDuplicates(primaryKey.COLUMN_NAME)\n\n\n#pdf['hex'] = pdf['hex'].apply(int, base=16)\n\n#pdf = pdf.sort_values(by=['hex','hex2'])\n#show(df_dedupe)"],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6493aec-0d44-4ab9-a6c2-4f2032a8063a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"application/vnd.livy.statement-meta+json":{"session_id":null,"spark_pool":null,"state":"cancelled","execution_finish_time":"2022-04-05T03:38:00.538927Z","execution_start_time":null,"queued_time":"2022-04-05T03:37:56.6276679Z","livy_statement_state":null,"session_start_time":"2022-04-05T03:37:56.8936651Z","statement_id":null},"text/plain":["StatementMeta(, , , Cancelled, )"]},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":null,"spark_pool":null,"state":"cancelled","execution_finish_time":"2022-04-05T03:38:00.538927Z","execution_start_time":null,"queued_time":"2022-04-05T03:37:56.6276679Z","livy_statement_state":null,"session_start_time":"2022-04-05T03:37:56.8936651Z","statement_id":null},"text/plain":["StatementMeta(, , , Cancelled, )"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0ceaf9c-310a-4ea4-b72b-a2a0dd758497"}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"name":"python"},"save_output":true,"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"synapse_widget":{"state":{},"version":"0.1"},"application/vnd.databricks.v1+notebook":{"notebookName":"DeltaProcessingNotebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3804581651847246}},"nbformat":4,"nbformat_minor":0}

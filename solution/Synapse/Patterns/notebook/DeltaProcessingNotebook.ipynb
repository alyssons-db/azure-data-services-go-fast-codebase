{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "TaskObject = \"\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.6083447Z",
              "session_start_time": "2022-08-01T04:34:49.7599532Z",
              "execution_start_time": "2022-08-01T04:37:40.8656769Z",
              "execution_finish_time": "2022-08-01T04:37:41.0238648Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
        "\n",
        "TaskObject = \"{\\\"TaskInstanceId\\\":44,\\\"TaskMasterId\\\":14,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"Azure Storage to Azure Storage\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"30cfca38-1e2f-4294-b1c3-66bac072439f\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ark-stg-kv-ads-aqye.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-2,\\\"EngineName\\\":\\\"arkstgsynwadsaqye\\\",\\\"SystemType\\\":\\\"Synapse\\\",\\\"ResourceGroup\\\":\\\"lockboxdev02\\\",\\\"SubscriptionId\\\":\\\"687fe1ae-a520-4f86-b921-a80664c40f9b\\\",\\\"ADFPipeline\\\":\\\"GPL_SparkNotebookExecution_Azure\\\",\\\"EngineJson\\\":\\\"{\\\\r\\\\n            \\\\\\\"endpoint\\\\\\\": \\\\\\\"https://arkstgsynwadsaqye.dev.azuresynapse.net\\\\\\\", \\\\\\\"DeltaProcessingNotebook\\\\\\\": \\\\\\\"DeltaProcessingNotebook\\\\\\\", \\\\\\\"PurviewAccountName\\\\\\\": \\\\\\\"arkstgpuradsaqye\\\\\\\", \\\\\\\"DefaultSparkPoolName\\\\\\\":\\\\\\\"arkstgsynspads\\\\\\\"\\\\r\\\\n        }\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\",\\\"JsonProperties\\\":{\\\"endpoint\\\":\\\"https://arkstgsynwadsaqye.dev.azuresynapse.net\\\",\\\"DeltaProcessingNotebook\\\":\\\"DeltaProcessingNotebook\\\",\\\"PurviewAccountName\\\":\\\"arkstgpuradsaqye\\\",\\\"DefaultSparkPoolName\\\":\\\"arkstgsynspads\\\"}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://arkstgdlsadsaqyeadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"SecretName\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"samples/\\\",\\\"TargetRelativePath\\\":\\\"/DF/\\\"},\\\"DataFileName\\\":\\\"userdata1.parquet\\\", \\\"MaxConcurrentConnections\\\":0,\\\"RelativePath\\\":\\\"samples/SalesLT_Customer_CDC/\\\",\\\"SchemaFileName\\\":\\\"userdata.json\\\",\\\"Type\\\":\\\"Parquet\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\",\\\"DeleteAfterCompletion\\\":\\\"false\\\",\\\"Recursively\\\":\\\"false\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://arkstgdlsadsaqyeadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"SecretName\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"samples/\\\",\\\"TargetRelativePath\\\":\\\"/DF\\\"},\\\"DataFileName\\\":\\\"TestDelta\\\",\\\"MaxConcurrentConnections\\\":0,\\\"RelativePath\\\":\\\"/Tests/Azure Storage to Azure Storage/-1001/\\\",\\\"SchemaFileName\\\":\\\"userdata.json\\\",\\\"Type\\\":\\\"Delta\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\",\\\"DeleteAfterCompletion\\\":\\\"false\\\",\\\"Recursively\\\":\\\"false\\\"},\\\"TMOptionals\\\":{\\\"CDCSource\\\":\\\"Disabled\\\",\\\"ModifiedDate\\\":\\\"Enabled\\\",\\\"Purview\\\":\\\"Disabled\\\",\\\"QualifiedIDAssociation\\\":\\\"TaskMasterId\\\",\\\"SparkTableCreate\\\":\\\"Enabled\\\",\\\"SparkTableDBName\\\":\\\"DBT\\\",\\\"SparkTableName\\\":\\\"TestTableforDF\\\",\\\"UseNotebookActivity\\\":\\\"Enabled\\\"}}\"\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "29",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-02T03:29:24.3428615Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-02T03:29:24.4419668Z",
              "execution_finish_time": "2022-08-02T03:29:24.5848435Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 29, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 99,
      "metadata": {
        "tags": [
          "parameters_overwritten"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "\n",
        "session_id = random.randint(0,1000000)\n",
        "\n",
        "TaskObjectJson = json.loads(TaskObject)\n",
        "Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "Schema = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "\n",
        "\n",
        "Source = Source + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['DataFileName']\n",
        "Schema = Schema + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['SchemaFileName']\n",
        "Target = Target + TaskObjectJson['Target']['Instance']['TargetRelativePath'] + \"/\" + TaskObjectJson['Target']['DataFileName']\n",
        "\n",
        "\n",
        "#remove any double slashes\n",
        "Source = Source.replace('//', '/')\n",
        "Schema = Schema.replace('//', '/')\n",
        "Target = Target.replace('//', '/')\n",
        "\n",
        "#get source and target data types\n",
        "SourceDT = TaskObjectJson['Source']['Type']\n",
        "TargetDT = TaskObjectJson['Target']['Type']\n",
        "\n",
        "#Check if we are adding a ModifiedDate Column, Note -> This is only available when parquet is the SourceDT\n",
        "if(SourceDT == 'Parquet'):\n",
        "    if(TaskObjectJson['TMOptionals']['ModifiedDate'] == 'Enabled'):\n",
        "        AddModifiedDate = True\n",
        "    else:\n",
        "        AddModifiedDate = False\n",
        "else:\n",
        "    AddModifiedDate = False\n",
        "\n",
        "AddModifiedDate = True\n",
        "#add abfss\n",
        "Source = \"abfss://\" + Source\n",
        "Schema = \"abfss://\" + Schema\n",
        "Target = \"abfss://\" + Target\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "Source = Source.replace(\"{yyyy}\", \"%Y\")\n",
        "Source = Source.replace(\"{MM}\", \"%m\")\n",
        "Source = Source.replace(\"{dd}\", \"%d\")\n",
        "Source = Source.replace(\"{hh}\", \"%H\")\n",
        "Source = Source.replace(\"{mm}\", \"%M\")\n",
        "Source = now.strftime(Source)\n",
        "\n",
        "Target = Target.replace(\"{yyyy}\", \"%Y\")\n",
        "Target = Target.replace(\"{MM}\", \"%m\")\n",
        "Target = Target.replace(\"{dd}\", \"%d\")\n",
        "Target = Target.replace(\"{hh}\", \"%H\")\n",
        "Target = Target.replace(\"{mm}\", \"%M\")\n",
        "Target = now.strftime(Target)\n",
        "\n",
        "Schema = Schema.replace(\"{yyyy}\", \"%Y\")\n",
        "Schema = Schema.replace(\"{MM}\", \"%m\")\n",
        "Schema = Schema.replace(\"{dd}\", \"%d\")\n",
        "Schema = Schema.replace(\"{hh}\", \"%H\")\n",
        "Schema = Schema.replace(\"{mm}\", \"%M\")\n",
        "Schema = now.strftime(Schema)\n",
        "\n",
        "print (\"Source: \" + Source)\n",
        "print (\"Schema: \" + Schema)\n",
        "print (\"Target: \" + Target)\n",
        "\n",
        "print (\"SourceDT: \" + SourceDT)\n",
        "print (\"TargetDT: \" + TargetDT)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "29",
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-02T04:00:17.7634774Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-02T04:00:17.8622601Z",
              "execution_finish_time": "2022-08-02T04:00:18.0206748Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 29, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: abfss://datalakeraw@arkstgdlsadsaqyeadsl.dfs.core.windows.net/samples/userdata1.parquet\nSchema: abfss://datalakeraw@arkstgdlsadsaqyeadsl.dfs.core.windows.net/samples/userdata.json\nTarget: abfss://datalakeraw@arkstgdlsadsaqyeadsl.dfs.core.windows.net/DF/TestDelta\nSourceDT: Parquet\nTargetDT: Delta\n"
          ]
        }
      ],
      "execution_count": 118,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(\"Schema found - using schema json to find PK\")\n",
        "    schema = spark.read.load(Schema, format='json', multiLine=True)\n",
        "    #convert it into a list so we can loop it using python rules\n",
        "    schema = schema.collect()\n",
        "    #loop through each column to find the primary key column\n",
        "    for col in schema:\n",
        "        if col.PKEY_COLUMN:\n",
        "            print(col.COLUMN_NAME)\n",
        "            primaryKey = col\n",
        "            partialMergeCondition = \"oldData.\" + primaryKey.COLUMN_NAME + \" = newData.\" + primaryKey.COLUMN_NAME \n",
        "            break\n",
        "except:\n",
        "    print(\"Schema json not found - assuming source dataframe first column is PK\")\n",
        "    df = spark.read.load(Source, format=SourceDT)\n",
        "    primaryKey = df.columns[0]\n",
        "    print(primaryKey)\n",
        "    partialMergeCondition = \"oldData.\" + primaryKey + \" = newData\"\n",
        "\n",
        "#set up the merge condition used in the next code block\n",
        "print(partialMergeCondition)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "29",
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-02T04:00:24.8511184Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-02T04:00:24.9514457Z",
              "execution_finish_time": "2022-08-02T04:00:26.0013866Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 29, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema found - using schema json to find PK\nid\noldData.id = newData.id\n"
          ]
        }
      ],
      "execution_count": 119,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "import pandas as pd\n",
        "from pyspark.sql import functions as F\n",
        "if(TaskObjectJson['TMOptionals']['CDCSource'] == 'Enabled'):\n",
        "    print(\"CDC Source\")\n",
        "    df = spark.read.load(Source, format=SourceDT)\n",
        "    #these are our cdc specific columns\n",
        "    cdcCols = [\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$update_mask\", \"__$operation\", \"__$command_id\"]\n",
        "    #we are dropping all of the $ / _ as they cause issues with spark SQL functions -> this may be changed to just remove the first 3 chars of each of the cdcCols (__$)\n",
        "    cdcColsToDrop = [\"startlsn\", \"endlsn\", \"seqval\", \"updatemask\", \"operation\", \"commandid\"]\n",
        "    #these are columns we want to convert from binary data types to string as dataframes do not play nice with them currently\n",
        "   #colsToString = [\"__$start_lsn\", \"__$end_lsn\", \"__$seqval\", \"__$update_mask\"]\n",
        "    colsToString = [\"startlsn\", \"endlsn\", \"seqval\", \"updatemask\"]\n",
        "\n",
        "    for col in cdcCols:\n",
        "        new_col = col.replace('_','')\n",
        "        new_col = new_col.replace('$','')\n",
        "        df = df.withColumnRenamed(col, new_col)\n",
        "\n",
        "    for col in colsToString: \n",
        "        try:\n",
        "            df = df.withColumn(col, hex(col))\n",
        "        except:\n",
        "            print(\"Error converting the column \" + col)\n",
        "\n",
        "    #convert to pandas dataframe so we can do more manipulation\n",
        "    pdf = df.toPandas()\n",
        "\n",
        "    #we want to sort by our start lsn and then by the seqval so that we can drop everything except the most recent database change for every unique row\n",
        "    try:\n",
        "        #columns we are sorting by, the LSN and then the sequence value - ensure the latest is at the bottom of the table\n",
        "        pdf = pdf.sort_values(by=[\"startlsn\", \"seqval\"])\n",
        "    except:\n",
        "        print(\"error in finding valid sorting columns - skipping.\")\n",
        "    pdf_dedupe = pdf.drop_duplicates(subset=[primaryKey.COLUMN_NAME], keep='last', inplace=False)\n",
        "\n",
        "    df = spark.createDataFrame(pdf_dedupe)\n",
        "\n",
        "    #operation 1 is equal to delete, the other 3 operations (inserts old and new / upserts) can be done together\n",
        "    dfDeletes = df.filter(\"operation == 1\")\n",
        "    dfUpserts = df.filter(\"operation != 1\")\n",
        "    #We want to sort our columns by our primary key now that we have only the latest actions\n",
        "    dfDeletes = dfDeletes.sort(primaryKey.COLUMN_NAME)\n",
        "    dfUpserts = dfUpserts.sort(primaryKey.COLUMN_NAME)\n",
        "\n",
        "    #drop unwanted columns -> not needed for our delta table as they are cdc specific\n",
        "    for col in cdcColsToDrop:\n",
        "        try:\n",
        "            dfDeletes = dfDeletes.drop(col)\n",
        "            dfUpserts = dfUpserts.drop(col)\n",
        "        except:\n",
        "            print(\"Error dropping the column \" + col)\n",
        "\n",
        "    mergeCondition = partialMergeCondition\n",
        "    sql = 'describe detail \"' + Target + '\"'\n",
        "    try:\n",
        "        if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "            print(\"Table already exists. Performing Merge\")\n",
        "            olddt = DeltaTable.forPath(spark, Target)  \n",
        "            olddt.alias(\"oldData\").merge(\n",
        "                dfUpserts.alias(\"newData\"),\n",
        "                mergeCondition) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()\n",
        "        else:\n",
        "            print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
        "            dfUpserts.write.format(\"delta\").save(Target)\n",
        "    except: \n",
        "        print(\"Table does not exist, error thrown. Creating new Delta Table. Note - this error can be that no file is found.\") \n",
        "        dfUpserts.write.format(\"delta\").save(Target)\n",
        "\n",
        "    olddt = DeltaTable.forPath(spark, Target)  \n",
        "    olddt.alias(\"oldData\").merge(\n",
        "    dfDeletes.alias(\"newData\"),\n",
        "    mergeCondition) \\\n",
        "    .whenMatchedDelete() \\\n",
        "    .execute()  \n",
        "\n",
        "else:\n",
        "    print(\"Non CDC Source\")\n",
        "    if(TargetDT == 'Delta'):\n",
        "        print(\"SourceDT = \" + SourceDT + \", TargetDT = Delta.\")\n",
        "        df = spark.read.load(Source, format=SourceDT)\n",
        "        if(SourceDT == 'Parquet' and AddModifiedDate == True):\n",
        "            print(\"Adding Modified Date column to source dataframe\")\n",
        "            df = df.withColumn('ModifiedDate', F.current_timestamp())\n",
        "        df = df.withColumn(\"row_sha2\", F.sha2(F.concat_ws(\"||\", F.array([c for c in df.columns if c not in {'ModifiedDate', 'row_sha2'}])), 256))\n",
        "        mergeCondition = partialMergeCondition + \" AND oldData.row_sha2 != newData.row_sha2\"\n",
        "        sql = 'describe detail \"' + Target + '\"'\n",
        "        try:\n",
        "            if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "                print(\"Table already exists. Performing Merge\")\n",
        "                olddt = DeltaTable.forPath(spark, Target)  \n",
        "                olddt.alias(\"oldData\").merge(\n",
        "                    df.alias(\"newData\"),\n",
        "                    mergeCondition) \\\n",
        "                .whenMatchedUpdateAll() \\\n",
        "                .whenNotMatchedInsertAll() \\\n",
        "                .execute()\n",
        "            else:\n",
        "                print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
        "                df.write.format(\"delta\").save(Target)\n",
        "        except:\n",
        "            print(\"Table does not exist. Error occured, creating new Delta Table.\")    \n",
        "            df.write.format(\"delta\").save(Target)\n",
        "    elif(TargetDT == 'Parquet' and SourceDT == 'Delta'):\n",
        "        print(\"SourceDT = Delta, TargetDT = Parquet.\")\n",
        "        df = DeltaTable.toDF(DeltaTable.forPath(spark, Source))\n",
        "        df.write.format(\"parquet\").mode(\"overwrite\").save(Target) "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "29",
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-02T04:00:28.8063153Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-02T04:00:28.940986Z",
              "execution_finish_time": "2022-08-02T04:00:32.8903046Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 29, 26, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non CDC Source\nSourceDT = Parquet, TargetDT = Delta.\nAdding Modified Date column to source dataframe\nTable does not exist. Creating new Delta Table.\n"
          ]
        }
      ],
      "execution_count": 120,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spark_table(df, targetDB, targetTable):\r\n",
        "    #we need to make the DB and table lowercase as synapse persistent tables dont identify them as different identities\r\n",
        "    targetDB = targetDB.lower()\r\n",
        "    targetTable = targetTable.lower()\r\n",
        "\r\n",
        "    #check if the specified DB / table exists - if so only do required actions.\r\n",
        "    dbList = spark.catalog.listDatabases()\r\n",
        "    dbExists = False\r\n",
        "    for db in dbList:\r\n",
        "        if (db.name == targetDB):\r\n",
        "            dbExists = True\r\n",
        "            break\r\n",
        "    if (dbExists):\r\n",
        "        print(\"DB Exists\")\r\n",
        "        tableExists = False\r\n",
        "        spark.catalog.setCurrentDatabase(targetDB)\r\n",
        "        tableList = spark.catalog.listTables()\r\n",
        "        for table in tableList:\r\n",
        "            if (table.name == targetTable):\r\n",
        "                tableExists = True\r\n",
        "                break\r\n",
        "        if (tableExists):\r\n",
        "            print(\"Table exists - nothing needed to be done\")\r\n",
        "            #spark.catalog.refreshTable(targetTable)\r\n",
        "            df.write.mode(\"overwrite\").saveAsTable(targetTable)\r\n",
        "        else:\r\n",
        "            print(\"Table doesnt exist - creating\")\r\n",
        "            #spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\r\n",
        "            df.write.mode(\"overwrite\").saveAsTable(targetTable)\r\n",
        "\r\n",
        "    else:\r\n",
        "        print(\"DB Doesnt exist - creating DB and table\")\r\n",
        "        createDBString = \"CREATE DATABASE \" + targetDB \r\n",
        "        spark.sql(createDBString)\r\n",
        "        spark.catalog.setCurrentDatabase(targetDB)\r\n",
        "        #spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\r\n",
        "        df.write.mode(\"overwrite\").saveAsTable(targetTable)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This checks if the user wants to save the sink as a persistent table\n",
        "if(TaskObjectJson['TMOptionals']['SparkTableCreate'] == 'Enabled'):\n",
        "    print(\"Creating Spark Table\")\n",
        "    sinkDF = DeltaTable.toDF(DeltaTable.forPath(spark, Target))\n",
        "    targetDB = TaskObjectJson['TMOptionals']['SparkTableDBName']\n",
        "    targetTable = TaskObjectJson['TMOptionals']['SparkTableName']\n",
        "\n",
        "    spark_table(sinkDF, targetDB, targetTable)\n",
        "#below has been changed, now writing directly from dataframe instead of using an external table.\n",
        "\n",
        "    #if the target datatype is parquet then we do not need to create a copy of the data - we can use the recently saved sink target\n",
        "    #if (TargetDT == 'Parquet'):\n",
        "    #   SnapshotTarget = Target\n",
        "    #else:\n",
        "    #    SnapshotTarget = Target + '/'+ TaskObjectJson['TMOptionals']['SparkTableDBName'] + '/' + TaskObjectJson['TMOptionals']['SparkTableName']\n",
        "    #    #we need to update the parquet file - this is not very efficient but there isnt a current better way as delta tables are not supported for persistent tables\n",
        "    #    df.write.format(\"parquet\").mode(\"overwrite\").save(SnapshotTarget)\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Spark Table creation\")\n",
        "\n",
        "#%%sql\n",
        "#CREATE TABLE testdb.dbo.test\n",
        "#USING PARQUET OPTIONS ('path'= 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT.Customer.chunk_2.parquet', 'inferschema'=true);\n",
        "#select * from testdb.dbo.test limit 10\n",
        "\n",
        "    \n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "29",
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-02T04:00:53.0770017Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-02T04:00:55.4093494Z",
              "execution_finish_time": "2022-08-02T04:01:00.7215901Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 29, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Spark Table\nDB Exists\nTable exists - nothing needed to be done\n"
          ]
        }
      ],
      "execution_count": 121,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#olddt.history().show(20, 1000, False)\n",
        "#display(spark.read.format(\"delta\").load(Target))\n",
        "#spark.sql(\"CREATE TABLE SalesLTCustomer USING DELTA LOCATION '{0}'\".format(TargetFile))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.66612Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-01T04:38:53.9089447Z",
              "execution_finish_time": "2022-08-01T04:38:54.0962426Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Execute Upsert\n",
        "#(old_deltaTable\n",
        "# .alias(\"oldData\") \n",
        "# .merge(newIncrementalData.alias(\"newData\"), \"oldData.id = newData.id\")\n",
        "# .whenMatchedUpdate(set = {\"name\": col(\"newData.name\")})\n",
        "# .whenNotMatchedInsert(values = {\"id\": col(\"newData.id\"), \"name\":\n",
        "#                                col(\"newData.name\")})\n",
        "# .execute()\n",
        "#)\n",
        "#\n",
        "# Display the records to check if the records are Merged\n",
        "#display(spark.read.format(\"delta\").load(Target))\n",
        "\n",
        "\n",
        "#df2 = spark.read.load(Target, format='delta')\n",
        "#display(df2)\n",
        "#print(type(tdf))\n",
        "#df.printSchema()\n",
        "#df = spark.read.load(Target, format=SourceDT)\n",
        "#display(df)\n",
        "#df = df.drop('row_sha2')\n",
        "#valueWhenTrue = None  \n",
        "##df = df.withColumn(\"comments\", when(df.id == 1,\"Male\").otherwise(df.comments))\n",
        "#df = df.withColumn(\"row_sha2\", F.sha2(F.concat_ws(\"||\", F.array([c for c in df.columns if c not in {'ModifiedDate', 'row_sha2'}])), 256))\n",
        "#display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.6765716Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-01T04:38:54.19559Z",
              "execution_finish_time": "2022-08-01T04:38:54.3552588Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#olddt.history().show(20, 1000, False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.6855571Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-01T04:38:54.4586529Z",
              "execution_finish_time": "2022-08-01T04:38:54.6124955Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "#NOTE -> This is an alternate way of upserting into delta table. Using manual method of getting each column required from the schema / dataframe and then creating a dictionary to use for the upsert.\n",
        "#           Currently not using this however it does work. Would advise to change the script to just create a dictionary and insert that instead of creating a string to convert into a dict.\n",
        "#########################\n",
        "#from delta.tables import *\n",
        "#df = spark.read.load(Source, format='parquet')\n",
        "#updatecols = []\n",
        "#insertcols = []\n",
        "#for col in schema:\n",
        "#    updatecols.append(col.COLUMN_NAME)\n",
        "#\n",
        "#for col in df.dtypes:\n",
        "#    insertcols.append(col[0])\n",
        "#\n",
        "#creating a string to be converted to dictionary \n",
        "#note -> can easily re-write this as just a dictionary if end up using this method.\n",
        "#updatestring = '{'\n",
        "#insertstring = '{'\n",
        "#\n",
        "#Go through each column in the schema to check what needs to be updated\n",
        "#for col in updatecols:\n",
        "#    updatestring = updatestring + '\"' + col + '\": \"newData.' + col +'\", '\n",
        "#updatestring = updatestring[:-2]\n",
        "#updatestring = updatestring + '}'\n",
        "#\n",
        "#Go through the new data to check what columns need to be inserted\n",
        "#for col in insertcols:\n",
        "#    insertstring = insertstring + '\"' + col + '\": \"newData.' + col +'\", '\n",
        "#insertstring = insertstring[:-2]\n",
        "#insertstring = insertstring + '}'\n",
        "#\n",
        "#print(updatestring)\n",
        "#print(insertstring)\n",
        "#\n",
        "#convert to dict\n",
        "#res = json.loads(updatestring)\n",
        "#res2 = json.loads(insertstring)\n",
        "#\n",
        "#sql = 'describe detail \"' + Target + '\"'\n",
        "#try:\n",
        "#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "#        print(\"Table already exists. Performing Merge\")\n",
        "#        olddt = DeltaTable.forPath(spark, Target)  \n",
        "#        olddt.alias(\"oldData\").merge(\n",
        "#            df.alias(\"newData\"),\n",
        "#            mergeCondition) \\\n",
        "#        .whenMatchedUpdate(set = res) \\\n",
        "#        .whenNotMatchedInsert(values = res2) \\\n",
        "#        .execute()\n",
        "#except:\n",
        "#    print(\"Table does not exist.\")    \n",
        "#    df.write.format(\"delta\").save(Target)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.694944Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-01T04:38:54.7204828Z",
              "execution_finish_time": "2022-08-01T04:38:54.8940469Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from delta.tables import * \n",
        "#import pandas as pd\n",
        "#Source = 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT.Customer.chunk_1.parquet'\n",
        "#Target = 'abfss://datalakeraw@arkstgdlsadsenrzadsl.dfs.core.windows.net/samples/SalesLT_Customer_Delta/SalesLT.Customer'\n",
        "#mergeCondition = \"oldData.\" + \"CustomerID\" + \" = newData.\" + \"CustomerID\"\n",
        "\n",
        "#df = spark.read.load(Source, format='parquet')\n",
        "\n",
        "#these are our cdc specific columns\n",
        "#cdcCols = ['__$start_lsn', '__$end_lsn', '__$seqval', '__$operation', '__$update_mask', '__$command_id']\n",
        "\n",
        "#convert to pandas dataframe so we can do more manipulation\n",
        "#pdf = df.toPandas()\n",
        "#we want to sort by our start lsn and then by the seqval so that we can drop everything except the most recent database change for every unique row\n",
        "#try:\n",
        "    #columns we are sorting by, the LSN and then the sequence value - ensure the latest is at the bottom of the table\n",
        "#    pdf = pdf.sort_values(by=['__$start_lsn', '__$seqval'])\n",
        "#except:\n",
        "#   print(\"error in finding valid sorting columns - skipping.\")\n",
        "\n",
        "#df = pdf.drop_duplicates(subset=['CustomerID'], keep='last', inplace=False)\n",
        "#df_dedupe = pdf.drop_duplicates(subset=[primaryKey.COLUMN_NAME], keep='last', inplace=False)\n",
        "\n",
        "#df = spark.createDataFrame(df)\n",
        "#dfDeletes = df.filter(\"CustomerID < 100\")\n",
        "#dfUpserts = df.filter(\"CustomerID >= 100\")\n",
        "#dfDeletes = df.filter(\"__$operation == 1\")\n",
        "#dfUpserts = df.filter(\"__$operation != 1\")\n",
        "\n",
        "#drop unwanted columns -> not needed for our delta table as they are cdc specific\n",
        "#for col in cdcCols:\n",
        "#    try:\n",
        "#        dfDeletes = dfDeletes.drop(col)\n",
        "#        dfUpserts = dfUpserts.drop(col)\n",
        "#    except:\n",
        "#        print(\"Error dropping the column \" + col)\n",
        "\n",
        "#sql = 'describe detail \"' + Target + '\"'\n",
        "#try:\n",
        "#    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "#        print(\"Table already exists. Performing Merge\")\n",
        "#        olddt = DeltaTable.forPath(spark, Target)  \n",
        "#        olddt.alias(\"oldData\").merge(\n",
        "#            dfUpserts.alias(\"newData\"),\n",
        "#           mergeCondition) \\\n",
        "#        .whenMatchedUpdateAll() \\\n",
        "#        .whenNotMatchedInsertAll() \\\n",
        "#        .execute()\n",
        "#    else:\n",
        "#        print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
        "#       dfUpserts.write.format(\"delta\").save(Target)\n",
        "#except: \n",
        "#    print(\"Table does not exist, error thrown. Creating new Delta Table.\")    \n",
        "#    dfUpserts.write.format(\"delta\").save(Target)\n",
        "\n",
        "#olddt = DeltaTable.forPath(spark, Target)  \n",
        "#olddt.alias(\"oldData\").merge(\n",
        "#dfDeletes.alias(\"newData\"),\n",
        "#mergeCondition) \\\n",
        "#.whenMatchedDelete() \\\n",
        "#.execute()  \n",
        "#display(dfDeletes)\n",
        "\n",
        "#df.write.format(\"delta\").save(Target)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.7043212Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-01T04:38:55.0022119Z",
              "execution_finish_time": "2022-08-01T04:38:55.1666058Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from delta.tables import * \n",
        "#import pandas as pd\n",
        "\n",
        "#df = spark.createDataFrame([\"0x0000019600000178002D\",\"0x0000019600000178002D\",\"0x0000019600000178002D\", \"0x0000019600000178002A\"], \"string\").toDF(\"hex\")\n",
        "#hex2 = ['0x00000194000000A80002', '0x00000A94000000A80002', '0x00000194000000B80004', '0x00000194000000B80000']\n",
        "\n",
        "#pdf = df.toPandas()\n",
        "#pdf['hex2'] = hex2\n",
        "\n",
        "\n",
        "#df = spark.createDataFrame(pdf)\n",
        "#df_dedupe = df.dropDuplicates('hex')\n",
        "#df_dedupe = df.dropDuplicates(primaryKey.COLUMN_NAME)\n",
        "\n",
        "\n",
        "#pdf['hex'] = pdf['hex'].apply(int, base=16)\n",
        "\n",
        "#pdf = pdf.sort_values(by=['hex','hex2'])\n",
        "#show(df_dedupe)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "arkstgsynspads",
              "session_id": "26",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-08-01T04:34:49.7132877Z",
              "session_start_time": null,
              "execution_start_time": "2022-08-01T04:38:55.2687539Z",
              "execution_finish_time": "2022-08-01T04:38:55.4301654Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(arkstgsynspads, 26, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}